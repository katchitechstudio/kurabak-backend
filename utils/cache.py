"""
Redis Cache Utility - PRODUCTION READY V4.5 ğŸš€
=======================================================
âœ… CONNECTION POOL: 50 baÄŸlantÄ± sÄ±nÄ±rÄ±nÄ± patlatmaz (max=20)
âœ… INFINITE TTL SUPPORT: ttl=0 gÃ¶nderilirse veri ASLA silinmez
âœ… TRIPLE FALLBACK: Redis â†’ RAM â†’ Disk (JSON dosyasÄ±)
âœ… THREAD-SAFE: Ã‡oklu worker/thread ortamÄ±nda gÃ¼venli
âœ… JSON SERIALIZATION: Verileri otomatik string/json yapar
âœ… DISK BACKUP: Restart sonrasÄ± veri kaybÄ±nÄ± Ã¶nler
âœ… AUTO-RECOVERY: Redis Ã§Ã¶kse bile disk'ten veriyi yÃ¼kler
âœ… get_redis_client() EXPORT: FCM notification desteÄŸi
âœ… CLEANUP SYSTEM: 7 gÃ¼nden eski backup'larÄ± otomatik sil
âœ… TIMEOUT FIX: Render Redis iÃ§in yeterli baÄŸlantÄ± sÃ¼resi (V4.5)
"""

import os
import json
import logging
import time
import threading
from typing import Optional, Any, Dict
from pathlib import Path
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# ======================================
# DISK BACKUP SÄ°STEMÄ° (YENÄ°!)
# ======================================

class DiskBackup:
    """
    Redis Ã§Ã¶kerse veya restart atarsa, kritik verileri
    disk'ten yÃ¼kleyen kurtarma sistemi.
    """
    def __init__(self):
        # Backup klasÃ¶rÃ¼ (proje root'unda)
        self.backup_dir = Path("data/cache_backup")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        self._lock = threading.Lock()
        
        logger.info(f"ğŸ“ Disk Backup klasÃ¶rÃ¼: {self.backup_dir.absolute()}")
    
    def save(self, key: str, data: Any) -> bool:
        """
        Kritik veriyi disk'e kaydet (JSON formatÄ±nda)
        """
        try:
            with self._lock:
                # GÃ¼venli dosya adÄ± oluÅŸtur (: ve / karakterlerini temizle)
                safe_key = key.replace(":", "_").replace("/", "_")
                file_path = self.backup_dir / f"{safe_key}.json"
                
                # JSON'a Ã§evir ve kaydet
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        'key': key,
                        'data': data,
                        'timestamp': time.time()
                    }, f, default=str, indent=2)
                
                return True
        except Exception as e:
            logger.error(f"âŒ Disk kayÄ±t hatasÄ± [{key}]: {e}")
            return False
    
    def load(self, key: str) -> Optional[Any]:
        """
        Disk'ten veriyi yÃ¼kle
        """
        try:
            with self._lock:
                safe_key = key.replace(":", "_").replace("/", "_")
                file_path = self.backup_dir / f"{safe_key}.json"
                
                if not file_path.exists():
                    return None
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    backup = json.load(f)
                    
                    # 24 saatten eski backup'larÄ± yÃ¼kleme
                    age = time.time() - backup.get('timestamp', 0)
                    if age > 86400:  # 24 saat = 86400 saniye
                        logger.warning(f"âš ï¸ [{key}] Disk backup'Ä± Ã§ok eski ({age/3600:.1f} saat)")
                        return None
                    
                    return backup.get('data')
        except Exception as e:
            logger.error(f"âŒ Disk okuma hatasÄ± [{key}]: {e}")
            return None
    
    def delete(self, key: str) -> bool:
        """
        Disk'ten backup'Ä± sil
        """
        try:
            with self._lock:
                safe_key = key.replace(":", "_").replace("/", "_")
                file_path = self.backup_dir / f"{safe_key}.json"
                
                if file_path.exists():
                    file_path.unlink()
                    return True
                return False
        except Exception as e:
            logger.error(f"âŒ Disk silme hatasÄ± [{key}]: {e}")
            return False
    
    def list_keys(self) -> list:
        """
        Disk'teki tÃ¼m backup key'lerini listele
        """
        try:
            with self._lock:
                files = self.backup_dir.glob("*.json")
                keys = []
                for f in files:
                    # Dosya adÄ±ndan key'i geri oluÅŸtur
                    key = f.stem.replace("_", ":")
                    keys.append(key)
                return keys
        except Exception as e:
            logger.error(f"âŒ Disk listeleme hatasÄ±: {e}")
            return []
    
    def cleanup_old_backups(self, max_age_days: int = 7) -> int:
        """
        ğŸ§¹ Eski backup dosyalarÄ±nÄ± temizle
        
        Args:
            max_age_days: KaÃ§ gÃ¼nden eski dosyalar silinsin (varsayÄ±lan 7)
            
        Returns:
            Silinen dosya sayÄ±sÄ±
        """
        try:
            with self._lock:
                deleted_count = 0
                cutoff_time = time.time() - (max_age_days * 86400)
                
                for file_path in self.backup_dir.glob("*.json"):
                    try:
                        # DosyayÄ± oku ve timestamp'ini kontrol et
                        with open(file_path, 'r', encoding='utf-8') as f:
                            backup = json.load(f)
                            timestamp = backup.get('timestamp', 0)
                        
                        # Eski mi?
                        if timestamp < cutoff_time:
                            file_path.unlink()
                            deleted_count += 1
                            age_days = (time.time() - timestamp) / 86400
                            logger.info(f"ğŸ—‘ï¸ Eski backup silindi: {file_path.name} ({age_days:.1f} gÃ¼n)")
                    
                    except Exception as e:
                        logger.warning(f"âš ï¸ Dosya temizleme hatasÄ± [{file_path.name}]: {e}")
                        continue
                
                if deleted_count > 0:
                    logger.info(f"âœ… {deleted_count} adet eski backup temizlendi!")
                
                return deleted_count
        
        except Exception as e:
            logger.error(f"âŒ Cleanup hatasÄ±: {e}")
            return 0
    
    def get_backup_stats(self) -> dict:
        """
        ğŸ“Š Backup istatistiklerini getir
        
        Returns:
            {
                'total_files': int,
                'total_size_mb': float,
                'oldest_backup': datetime,
                'newest_backup': datetime
            }
        """
        try:
            with self._lock:
                files = list(self.backup_dir.glob("*.json"))
                
                if not files:
                    return {
                        'total_files': 0,
                        'total_size_mb': 0,
                        'oldest_backup': None,
                        'newest_backup': None
                    }
                
                total_size = sum(f.stat().st_size for f in files)
                timestamps = []
                
                for file_path in files:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            backup = json.load(f)
                            timestamps.append(backup.get('timestamp', 0))
                    except:
                        continue
                
                return {
                    'total_files': len(files),
                    'total_size_mb': round(total_size / (1024 * 1024), 2),
                    'oldest_backup': datetime.fromtimestamp(min(timestamps)) if timestamps else None,
                    'newest_backup': datetime.fromtimestamp(max(timestamps)) if timestamps else None
                }
        
        except Exception as e:
            logger.error(f"âŒ Stats hatasÄ±: {e}")
            return {'total_files': 0, 'total_size_mb': 0, 'oldest_backup': None, 'newest_backup': None}

# Global Disk Backup
disk_backup = DiskBackup()

# ======================================
# REDIS CLIENT WRAPPER (CONNECTION POOL)
# ======================================

class RedisClient:
    """
    Hata korumalÄ±, Connection Pool ile yÃ¶netilen Redis istemcisi.
    ğŸ”¥ YENÄ°: max_connections=20 ile 50 sÄ±nÄ±rÄ±nÄ± aÅŸmaz!
    ğŸ”¥ V4.5: Timeout'lar Render iÃ§in optimize edildi!
    """
    def __init__(self):
        self._client = None
        self._pool = None
        self._lock = threading.Lock()
        self._enabled = False
        self._connection_error_logged = False
        
        # Redis URL kontrolÃ¼ (Env'den gelir)
        self.redis_url = os.environ.get("REDIS_URL")

    def _connect(self):
        """Redis'e Connection Pool ile baÄŸlanÄ±r"""
        if not self.redis_url:
            if not self._connection_error_logged:
                logger.warning("âš ï¸ REDIS_URL tanÄ±mlÄ± deÄŸil! RAM + Disk Cache kullanÄ±lacak.")
                self._connection_error_logged = True
            return None

        try:
            import redis
            
            # ğŸ”¥ CONNECTION POOL (Hayati Ã–nem!) - V4.5 TIMEOUT FIX
            self._pool = redis.ConnectionPool.from_url(
                self.redis_url,
                max_connections=20,  # ğŸš¨ SÄ°HÄ°RLÄ° AYAR
                decode_responses=True,
                socket_connect_timeout=10,  # âœ… 3â†’10 saniye (Render iÃ§in)
                socket_timeout=10,  # âœ… 3â†’10 saniye  
                retry_on_timeout=True,  # âœ… Timeout'ta tekrar dene
                socket_keepalive=True,  # âœ… BaÄŸlantÄ±yÄ± canlÄ± tut
                socket_keepalive_options={
                    6: 1,   # TCP_KEEPIDLE = 60 saniye
                    5: 10,  # TCP_KEEPINTVL = 10 saniye
                    4: 3    # TCP_KEEPCNT = 3 deneme
                }
            )
            
            # Pool'dan client oluÅŸtur
            client = redis.Redis(connection_pool=self._pool)
            
            # Test et (10 saniye timeout ile)
            client.ping()
            
            logger.info("âœ… Redis Connection Pool baÅŸarÄ±lÄ±. (Max: 20 baÄŸlantÄ±, Timeout: 10s)")
            self._enabled = True
            return client
            
        except ImportError:
            if not self._connection_error_logged:
                logger.error("âŒ 'redis' kÃ¼tÃ¼phanesi eksik! (pip install redis)")
                self._connection_error_logged = True
            return None
        except Exception as e:
            if not self._connection_error_logged:
                logger.error(f"âŒ Redis baÄŸlantÄ± hatasÄ±: {e}")
                logger.error(f"   Redis URL: {self.redis_url[:30]}...")  # Ä°lk 30 karakter
                self._connection_error_logged = True
            return None

    def get_client(self):
        """Lazy connection: Ä°lk ihtiyaÃ§ duyulduÄŸunda baÄŸlanÄ±r"""
        if self._client:
            return self._client
            
        with self._lock:
            if not self._client:
                self._client = self._connect()
            return self._client

    def is_enabled(self):
        return self._enabled

# Global Redis Wrapper
redis_wrapper = RedisClient()

# ======================================
# RAM CACHE (FALLBACK)
# ======================================

class RAMCache:
    """
    Redis yoksa devreye giren basit bellek deposu.
    """
    def __init__(self):
        self._cache: Dict[str, Any] = {}
        self._lock = threading.Lock()

    def set(self, key: str, value: Any, ttl: int = 0):
        with self._lock:
            expiry = time.time() + ttl if ttl > 0 else 0  # 0 ise sonsuz
            self._cache[key] = (value, expiry)

    def get(self, key: str):
        with self._lock:
            if key not in self._cache:
                return None
            
            value, expiry = self._cache[key]
            
            # SÃ¼re dolmuÅŸ mu? (Expiry 0 ise dolmaz)
            if expiry > 0 and time.time() > expiry:
                del self._cache[key]
                return None
                
            return value
    
    def exists(self, key: str) -> bool:
        """Key var mÄ± kontrol et"""
        with self._lock:
            if key not in self._cache:
                return False
            
            value, expiry = self._cache[key]
            
            # SÃ¼re dolmuÅŸsa False dÃ¶ndÃ¼r
            if expiry > 0 and time.time() > expiry:
                del self._cache[key]
                return False
            
            return True
    
    def delete(self, key: str) -> bool:
        """Key'i sil"""
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                return True
            return False
    
    def keys(self, pattern: str = "*"):
        """Pattern'e uyan tÃ¼m key'leri dÃ¶ndÃ¼r"""
        with self._lock:
            if pattern == "*":
                return list(self._cache.keys())
            
            # Basit wildcard desteÄŸi
            import fnmatch
            return [k for k in self._cache.keys() if fnmatch.fnmatch(k, pattern)]

# Global RAM Cache
ram_cache = RAMCache()

# ======================================
# KRÄ°TÄ°K VERÄ° LÄ°STESÄ°
# ======================================

# Bu key'ler disk'e de yedeklenir (Restart sonrasÄ± kurtarma iÃ§in)
CRITICAL_KEYS = [
    'kurabak:currencies:all',
    'kurabak:golds:all',
    'kurabak:silvers:all',
    'kurabak:summary',
    'kurabak:yesterday_prices',  # Snapshot (en kritik!)
    'kurabak:backup:all'
]

# ======================================
# PUBLIC API (DIÅARIYA AÃ‡ILAN FONKSÄ°YONLAR)
# ======================================

def get_cache(key: str) -> Optional[Any]:
    """
    TRIPLE FALLBACK: Redis â†’ RAM â†’ Disk
    """
    client = redis_wrapper.get_client()
    
    # 1. Redis Denemesi
    if client:
        try:
            data = client.get(key)
            if data:
                return json.loads(data)
        except Exception as e:
            logger.warning(f"âš ï¸ Redis Okuma HatasÄ±: {e} -> RAM'e geÃ§iliyor.")
    
    # 2. RAM Denemesi (Fallback)
    ram_data = ram_cache.get(key)
    if ram_data:
        return ram_data
    
    # 3. Disk Denemesi (Final Kurtarma!)
    if key in CRITICAL_KEYS:
        logger.warning(f"ğŸ”¥ [{key}] Redis ve RAM'de yok, DISK'ten yÃ¼kleniyor...")
        disk_data = disk_backup.load(key)
        if disk_data:
            logger.info(f"âœ… [{key}] Disk'ten baÅŸarÄ±yla kurtarÄ±ldÄ±!")
            # KurtarÄ±lan veriyi RAM'e de yÃ¼kle
            ram_cache.set(key, disk_data, ttl=0)
            return disk_data
    
    return None


def set_cache(key: str, data: Any, ttl: int = 300) -> bool:
    """
    Cache'e veri yazar + Kritik verileri disk'e yedekler
    """
    success = False
    
    # Veriyi JSON string'e Ã§evir
    try:
        json_data = json.dumps(data, default=str)
    except Exception as e:
        logger.error(f"âŒ JSON Serialization HatasÄ±: {e}")
        return False

    client = redis_wrapper.get_client()

    # 1. Redis Yazma
    if client:
        try:
            if ttl and ttl > 0:
                client.setex(key, ttl, json_data)  # SÃ¼reli kayÄ±t
            else:
                client.set(key, json_data)  # ğŸ”¥ SÃœRESÄ°Z KAYIT (ttl=0)
            success = True
        except Exception as e:
            logger.error(f"âŒ Redis Yazma HatasÄ±: {e}")

    # 2. RAM Yazma (Her zaman yedek olarak yazalÄ±m)
    ram_cache.set(key, data, ttl)
    
    # 3. ğŸ”¥ DÄ°SK YEDEKLEME (Sadece kritik veriler iÃ§in)
    if key in CRITICAL_KEYS:
        disk_backup.save(key, data)
        logger.debug(f"ğŸ’¾ [{key}] Disk'e yedeklendi")
    
    return success or True  # RAM'e yazÄ±ldÄ±ysa baÅŸarÄ±lÄ± say


def cache_exists(key: str) -> bool:
    """
    Key var mÄ± kontrol et (Redis â†’ RAM â†’ Disk)
    """
    client = redis_wrapper.get_client()
    
    # 1. Redis KontrolÃ¼
    if client:
        try:
            return bool(client.exists(key))
        except Exception as e:
            logger.warning(f"âš ï¸ Redis EXISTS hatasÄ±: {e}")
    
    # 2. RAM KontrolÃ¼
    if ram_cache.exists(key):
        return True
    
    # 3. Disk KontrolÃ¼ (Kritik key'ler iÃ§in)
    if key in CRITICAL_KEYS:
        return disk_backup.load(key) is not None
    
    return False


def delete_cache(key: str) -> bool:
    """
    Key'i sil (Redis + RAM + Disk)
    """
    success = False
    client = redis_wrapper.get_client()
    
    # 1. Redis Silme
    if client:
        try:
            client.delete(key)
            success = True
        except Exception as e:
            logger.warning(f"âš ï¸ Redis DELETE hatasÄ±: {e}")
    
    # 2. RAM Silme
    ram_cache.delete(key)
    
    # 3. Disk Silme (Kritik key'ler iÃ§in)
    if key in CRITICAL_KEYS:
        disk_backup.delete(key)
    
    return success or True


def get_cache_keys(pattern: str = "*"):
    """
    Pattern'e uyan tÃ¼m key'leri dÃ¶ndÃ¼r
    """
    client = redis_wrapper.get_client()
    
    # 1. Redis Denemesi
    if client:
        try:
            return [k.decode() if isinstance(k, bytes) else k 
                    for k in client.keys(pattern)]
        except Exception as e:
            logger.warning(f"âš ï¸ Redis KEYS hatasÄ±: {e}")
    
    # 2. RAM Denemesi
    ram_keys = ram_cache.keys(pattern)
    
    # 3. Disk'teki kritik key'leri de ekle
    disk_keys = disk_backup.list_keys()
    
    # Unique key listesi oluÅŸtur
    all_keys = set(ram_keys + disk_keys)
    
    # Pattern ile filtrele
    if pattern != "*":
        import fnmatch
        all_keys = {k for k in all_keys if fnmatch.fnmatch(k, pattern)}
    
    return list(all_keys)


def flush_all_cache() -> bool:
    """
    TÃœM cache'i temizle (Redis + RAM + Disk)
    âš ï¸ DÄ°KKAT: Bu komutu sadece Åef kullanmalÄ±!
    """
    success = False
    client = redis_wrapper.get_client()
    
    # 1. Redis TemizliÄŸi
    if client:
        try:
            client.flushall()
            logger.warning("ğŸ§¹ Redis tamamen temizlendi!")
            success = True
        except Exception as e:
            logger.error(f"âŒ Redis FLUSHALL hatasÄ±: {e}")
    
    # 2. RAM TemizliÄŸi
    ram_cache._cache.clear()
    logger.warning("ğŸ§¹ RAM Cache temizlendi!")
    
    # 3. Disk TemizliÄŸi (Kritik key'leri sil)
    for key in CRITICAL_KEYS:
        disk_backup.delete(key)
    logger.warning("ğŸ§¹ Disk Backup temizlendi!")
    
    return success or True


# ======================================
# ğŸ§¹ TEMÄ°ZLÄ°K FONKSÄ°YONU (PUBLIC API)
# ======================================

def cleanup_old_disk_backups(max_age_days: int = 7) -> dict:
    """
    ğŸ§¹ Eski disk backup'larÄ±nÄ± temizle
    
    Args:
        max_age_days: KaÃ§ gÃ¼nden eski dosyalar silinsin (varsayÄ±lan 7)
        
    Returns:
        {
            'deleted_count': int,
            'before_stats': dict,
            'after_stats': dict
        }
    """
    # Ã–nceki durum
    before_stats = disk_backup.get_backup_stats()
    
    # Temizlik yap
    deleted_count = disk_backup.cleanup_old_backups(max_age_days)
    
    # Sonraki durum
    after_stats = disk_backup.get_backup_stats()
    
    return {
        'deleted_count': deleted_count,
        'before_stats': before_stats,
        'after_stats': after_stats
    }


def get_disk_backup_stats() -> dict:
    """
    ğŸ“Š Disk backup istatistiklerini getir
    """
    return disk_backup.get_backup_stats()


# ======================================
# ğŸ”¥ FCM NOTIFICATION SUPPORT
# ======================================

def get_redis_client():
    """
    Redis client'Ä± dÃ¶ndÃ¼r
    
    Bu fonksiyon notification_service.py tarafÄ±ndan kullanÄ±lÄ±r.
    FCM token'larÄ±nÄ± Redis Set'inde saklamak iÃ§in gerekli.
    
    Returns:
        Redis client instance veya None
    """
    return redis_wrapper.get_client()


# ======================================
# STARTUP: DISK'TEN VERÄ° KURTARMA
# ======================================

def recover_from_disk():
    """
    Uygulama baÅŸlatÄ±lÄ±rken disk'ten kritik verileri yÃ¼kle
    (Redis Ã§Ã¶kmÃ¼ÅŸse veya restart atmÄ±ÅŸsa)
    """
    logger.info("ğŸ”„ Disk'ten veri kurtarma kontrolÃ¼ baÅŸlatÄ±lÄ±yor...")
    
    recovered_count = 0
    
    for key in CRITICAL_KEYS:
        # EÄŸer Redis ve RAM'de yoksa disk'ten yÃ¼kle
        if not get_cache(key):
            disk_data = disk_backup.load(key)
            if disk_data:
                logger.info(f"ğŸ’¾ [{key}] Disk'ten kurtarÄ±ldÄ± ve RAM'e yÃ¼klendi")
                ram_cache.set(key, disk_data, ttl=0)
                recovered_count += 1
    
    if recovered_count > 0:
        logger.info(f"âœ… {recovered_count} adet veri disk'ten baÅŸarÄ±yla kurtarÄ±ldÄ±!")
    else:
        logger.info("â„¹ï¸ KurtarÄ±lacak veri bulunamadÄ± (Normal durum)")

# Uygulama baÅŸlarken otomatik kurtarma yap
recover_from_disk()
