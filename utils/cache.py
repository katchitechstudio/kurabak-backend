"""
Redis Cache Utility - PRODUCTION READY V5.6 ğŸš€
=========================================================
âœ… CONNECTION POOL FIX: Global client kullanÄ±mÄ± (V4.8)
âœ… RAM CACHE CLEANUP: Otomatik Ã§Ã¶p toplama (V4.8)
âœ… DISK BACKUP OPTÄ°MÄ°ZE: Sadece kritik anlarda kaydet (V4.8)
âœ… INFINITE TTL SUPPORT: ttl=0 gÃ¶nderilirse veri ASLA silinmez
âœ… TRIPLE FALLBACK: Redis â†’ RAM â†’ Disk (JSON dosyasÄ±)
âœ… THREAD-SAFE: Ã‡oklu worker/thread ortamÄ±nda gÃ¼venli
âœ… JSON SERIALIZATION: Verileri otomatik string/json yapar
âœ… AUTO-RECOVERY: Redis Ã§Ã¶kse bile disk'ten veriyi yÃ¼kler
âœ… get_redis_client() EXPORT: FCM notification desteÄŸi
âœ… CLEANUP SYSTEM: 7 gÃ¼nden eski backup'larÄ± otomatik sil
âœ… TIMEOUT FIX: Render Redis iÃ§in yeterli baÄŸlantÄ± sÃ¼resi
âœ… EAGER CONNECTION: Startup'ta hemen baÄŸlan
âœ… ATOMIC INCR: Race Condition Ã¶nleme iÃ§in atomik increment
âœ… ğŸ”¥ RAM CLEANUP INTERVAL: 10 dakika (RAM OPTÄ°MÄ°ZASYON - V4.8.1)
âœ… ğŸ”¥ V5.5 SNAPSHOT KEYS: raw_snapshot + jeweler_snapshot (Disk backup desteÄŸi)
âœ… ğŸ”¥ V5.6 SCHEDULER LOCK: renew_scheduler_lock buraya taÅŸÄ±ndÄ± (circular import fix)
   app.py â†’ maintenance_service.py â†’ app.py dÃ¶ngÃ¼sÃ¼ kÄ±rÄ±ldÄ±.

V5.6 DeÄŸiÅŸiklikler:
- ğŸ”¥ SCHEDULER_LOCK_KEY, SCHEDULER_LOCK_TTL, renew_scheduler_lock() eklendi
- app.py ve maintenance_service.py artÄ±k buradan import eder
"""

import os
import json
import logging
import time
import threading
from typing import Optional, Any, Dict
from pathlib import Path
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

# ======================================
# DISK BACKUP SÄ°STEMÄ°
# ======================================

class DiskBackup:
    """
    Redis Ã§Ã¶kerse veya restart atarsa, kritik verileri
    disk'ten yÃ¼kleyen kurtarma sistemi.
    """
    def __init__(self):
        self.backup_dir = Path("data/cache_backup")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        self._lock = threading.Lock()
        
        logger.info(f"ğŸ“ Disk Backup klasÃ¶rÃ¼: {self.backup_dir.absolute()}")
    
    def save(self, key: str, data: Any) -> bool:
        """Kritik veriyi disk'e kaydet (JSON formatÄ±nda)"""
        try:
            with self._lock:
                safe_key = key.replace(":", "_").replace("/", "_")
                file_path = self.backup_dir / f"{safe_key}.json"
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump({
                        'key': key,
                        'data': data,
                        'timestamp': time.time()
                    }, f, default=str, indent=2)
                
                return True
        except Exception as e:
            logger.error(f"âŒ Disk kayÄ±t hatasÄ± [{key}]: {e}")
            return False
    
    def load(self, key: str) -> Optional[Any]:
        """Disk'ten veriyi yÃ¼kle"""
        try:
            with self._lock:
                safe_key = key.replace(":", "_").replace("/", "_")
                file_path = self.backup_dir / f"{safe_key}.json"
                
                if not file_path.exists():
                    return None
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    backup = json.load(f)
                    
                    age = time.time() - backup.get('timestamp', 0)
                    if age > 86400:
                        logger.warning(f"âš ï¸ [{key}] Disk backup'Ä± Ã§ok eski ({age/3600:.1f} saat)")
                        return None
                    
                    return backup.get('data')
        except Exception as e:
            logger.error(f"âŒ Disk okuma hatasÄ± [{key}]: {e}")
            return None
    
    def delete(self, key: str) -> bool:
        """Disk'ten backup'Ä± sil"""
        try:
            with self._lock:
                safe_key = key.replace(":", "_").replace("/", "_")
                file_path = self.backup_dir / f"{safe_key}.json"
                
                if file_path.exists():
                    file_path.unlink()
                    return True
                return False
        except Exception as e:
            logger.error(f"âŒ Disk silme hatasÄ± [{key}]: {e}")
            return False
    
    def list_keys(self) -> list:
        """Disk'teki tÃ¼m backup key'lerini listele"""
        try:
            with self._lock:
                files = self.backup_dir.glob("*.json")
                keys = []
                for f in files:
                    key = f.stem.replace("_", ":")
                    keys.append(key)
                return keys
        except Exception as e:
            logger.error(f"âŒ Disk listeleme hatasÄ±: {e}")
            return []
    
    def cleanup_old_backups(self, max_age_days: int = 7) -> int:
        """
        ğŸ§¹ Eski backup dosyalarÄ±nÄ± temizle
        
        Args:
            max_age_days: KaÃ§ gÃ¼nden eski dosyalar silinsin (varsayÄ±lan 7)
            
        Returns:
            Silinen dosya sayÄ±sÄ±
        """
        try:
            with self._lock:
                deleted_count = 0
                cutoff_time = time.time() - (max_age_days * 86400)
                
                for file_path in self.backup_dir.glob("*.json"):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            backup = json.load(f)
                            timestamp = backup.get('timestamp', 0)
                        
                        if timestamp < cutoff_time:
                            file_path.unlink()
                            deleted_count += 1
                            age_days = (time.time() - timestamp) / 86400
                            logger.info(f"ğŸ—‘ï¸ Eski backup silindi: {file_path.name} ({age_days:.1f} gÃ¼n)")
                    
                    except Exception as e:
                        logger.warning(f"âš ï¸ Dosya temizleme hatasÄ± [{file_path.name}]: {e}")
                        continue
                
                if deleted_count > 0:
                    logger.info(f"âœ… {deleted_count} adet eski backup temizlendi!")
                
                return deleted_count
        
        except Exception as e:
            logger.error(f"âŒ Cleanup hatasÄ±: {e}")
            return 0
    
    def get_backup_stats(self) -> dict:
        """ğŸ“Š Backup istatistiklerini getir"""
        try:
            with self._lock:
                files = list(self.backup_dir.glob("*.json"))
                
                if not files:
                    return {
                        'total_files': 0,
                        'total_size_mb': 0,
                        'oldest_backup': None,
                        'newest_backup': None
                    }
                
                total_size = sum(f.stat().st_size for f in files)
                timestamps = []
                
                for file_path in files:
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            backup = json.load(f)
                            timestamps.append(backup.get('timestamp', 0))
                    except:
                        continue
                
                return {
                    'total_files': len(files),
                    'total_size_mb': round(total_size / (1024 * 1024), 2),
                    'oldest_backup': datetime.fromtimestamp(min(timestamps)) if timestamps else None,
                    'newest_backup': datetime.fromtimestamp(max(timestamps)) if timestamps else None
                }
        
        except Exception as e:
            logger.error(f"âŒ Stats hatasÄ±: {e}")
            return {'total_files': 0, 'total_size_mb': 0, 'oldest_backup': None, 'newest_backup': None}

disk_backup = DiskBackup()

# ======================================
# ğŸ”¥ V4.8: REDIS CLIENT (CONNECTION LEAK FÄ°X!)
# ======================================

class RedisClient:
    """
    ğŸ”¥ V4.8 FIX: Global client kullanÄ±mÄ±
    
    Ã–NCEKÄ° SORUN:
    - Her get_cache() Ã§aÄŸrÄ±sÄ±nda yeni connection alÄ±nÄ±yordu
    - Connection'lar geri verilmiyordu
    - Pool doluyordu ve RAM'de birikiyor
    
    YENÄ° Ã‡Ã–ZÃœM:
    - Tek bir global Redis client
    - Connection pool otomatik yÃ¶netiliyor
    - Memory leak yok!
    """
    def __init__(self):
        self._client = None
        self._pool = None
        self._lock = threading.Lock()
        self._enabled = False
        self._connection_error_logged = False
        
        self.redis_url = os.environ.get("REDIS_URL")
        
        if self.redis_url:
            logger.info(f"ğŸ” [INIT] Redis URL bulundu, baÄŸlantÄ± kuruluyor...")
            self._client = self._connect()
        else:
            logger.warning("âš ï¸ [INIT] REDIS_URL yok, RAM + Disk kullanÄ±lacak")

    def _connect(self):
        """Redis'e Connection Pool ile baÄŸlanÄ±r"""
        if not self.redis_url:
            if not self._connection_error_logged:
                logger.warning("âš ï¸ REDIS_URL tanÄ±mlÄ± deÄŸil! RAM + Disk Cache kullanÄ±lacak.")
                self._connection_error_logged = True
            return None

        try:
            import redis
            
            logger.info(f"ğŸ” [CONNECT] redis modÃ¼lÃ¼ import edildi (v{redis.__version__})")
            
            # CONNECTION POOL
            self._pool = redis.ConnectionPool.from_url(
                self.redis_url,
                max_connections=20,
                decode_responses=True,
                socket_connect_timeout=10,
                socket_timeout=10,
                retry_on_timeout=True,
                socket_keepalive=True,
                socket_keepalive_options={
                    6: 1,
                    5: 10,
                    4: 3
                }
            )
            
            logger.info("ğŸ” [CONNECT] Connection pool oluÅŸturuldu")
            
            # ğŸ”¥ V4.8: GLOBAL CLIENT - SADECE BÄ°R KERE OLUÅTUR!
            client = redis.Redis(connection_pool=self._pool)
            
            logger.info("ğŸ” [CONNECT] Redis client oluÅŸturuldu, ping atÄ±lÄ±yor...")
            
            client.ping()
            
            logger.info("âœ… Redis baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±! (Global client kullanÄ±mda)")
            self._enabled = True
            return client
            
        except ImportError:
            if not self._connection_error_logged:
                logger.error("âŒ 'redis' kÃ¼tÃ¼phanesi eksik! (pip install redis)")
                self._connection_error_logged = True
            return None
        except Exception as e:
            if not self._connection_error_logged:
                logger.error(f"âŒ Redis baÄŸlantÄ± hatasÄ±: {e}")
                self._connection_error_logged = True
            return None

    def get_client(self):
        """
        ğŸ”¥ V4.8: Global client dÃ¶ndÃ¼r (Yeni connection AÃ‡MA!)
        """
        return self._client

    def is_enabled(self):
        return self._enabled

redis_wrapper = RedisClient()

# ======================================
# ğŸ”¥ V4.8.1: RAM CACHE (RAM OPTÄ°MÄ°ZASYON!)
# ======================================

class RAMCache:
    """
    ğŸ”¥ V4.8.1 OPTIMIZATION: RAM temizlik aralÄ±ÄŸÄ± artÄ±rÄ±ldÄ±
    
    Ã–NCEKÄ° SORUN:
    - Her 5 dakikada cleanup â†’ Gereksiz CPU/RAM kullanÄ±mÄ±
    
    YENÄ° Ã‡Ã–ZÃœM:
    - Her 10 dakikada cleanup â†’ %50 daha az kaynak tÃ¼ketimi
    - Memory leak yine yok, ama daha verimli!
    """
    def __init__(self):
        self._cache: Dict[str, Any] = {}
        self._lock = threading.Lock()
        
        # ğŸ”¥ V4.8.1: OTOMATIK TEMÄ°ZLÄ°K THREAD'Ä° (10 DK)
        self._cleanup_thread = threading.Thread(
            target=self._auto_cleanup,
            daemon=True,
            name="RAMCacheCleanup"
        )
        self._cleanup_thread.start()
        logger.info("ğŸ§¹ RAM Cache otomatik temizlik thread'i baÅŸlatÄ±ldÄ± (10dk interval)")

    def _auto_cleanup(self):
        """
        ğŸ§¹ Arka planda Ã§alÄ±ÅŸan temizlik thread'i
        
        ğŸ”¥ V4.8.1: Her 10 dakikada bir expired key'leri temizler (eski: 5dk)
        """
        while True:
            try:
                time.sleep(600)  # ğŸ”¥ 10 dakika bekle (eski: 300)
                
                with self._lock:
                    current_time = time.time()
                    keys_to_delete = []
                    
                    # Expired key'leri bul
                    for key, (value, expiry) in self._cache.items():
                        if expiry > 0 and current_time > expiry:
                            keys_to_delete.append(key)
                    
                    # Temizle
                    for key in keys_to_delete:
                        del self._cache[key]
                    
                    if keys_to_delete:
                        logger.info(f"ğŸ§¹ RAM Cache temizlendi: {len(keys_to_delete)} expired key silindi")
                
            except Exception as e:
                logger.error(f"âŒ RAM Cache cleanup hatasÄ±: {e}")
                time.sleep(60)  # Hata durumunda 1 dakika bekle

    def set(self, key: str, value: Any, ttl: int = 0):
        with self._lock:
            expiry = time.time() + ttl if ttl > 0 else 0
            self._cache[key] = (value, expiry)

    def get(self, key: str):
        with self._lock:
            if key not in self._cache:
                return None
            
            value, expiry = self._cache[key]
            
            if expiry > 0 and time.time() > expiry:
                del self._cache[key]
                return None
                
            return value
    
    def exists(self, key: str) -> bool:
        with self._lock:
            if key not in self._cache:
                return False
            
            value, expiry = self._cache[key]
            
            if expiry > 0 and time.time() > expiry:
                del self._cache[key]
                return False
            
            return True
    
    def delete(self, key: str) -> bool:
        with self._lock:
            if key in self._cache:
                del self._cache[key]
                return True
            return False
    
    def incr(self, key: str, ttl: int = 0) -> int:
        """Atomik increment (RAM iÃ§in thread-safe)"""
        with self._lock:
            current_value = 0
            
            if key in self._cache:
                value, expiry = self._cache[key]
                
                if expiry == 0 or time.time() <= expiry:
                    current_value = int(value) if isinstance(value, (int, str)) else 0
                else:
                    del self._cache[key]
            
            new_value = current_value + 1
            
            expiry = time.time() + ttl if ttl > 0 else 0
            self._cache[key] = (new_value, expiry)
            
            return new_value
    
    def keys(self, pattern: str = "*"):
        with self._lock:
            if pattern == "*":
                return list(self._cache.keys())
            
            import fnmatch
            return [k for k in self._cache.keys() if fnmatch.fnmatch(k, pattern)]

ram_cache = RAMCache()

# ======================================
# ğŸ”¥ V5.5: KRÄ°TÄ°K VERÄ° LÄ°STESÄ° (SNAPSHOT KEYS GÃœNCELLENDI!)
# ======================================

CRITICAL_KEYS = [
    'kurabak:currencies:all',
    'kurabak:golds:all',
    'kurabak:silvers:all',
    'kurabak:summary',
    'kurabak:raw_snapshot',      # ğŸ”¥ V5.5: Ham fiyat snapshot'Ä± (disk backup!)
    'kurabak:jeweler_snapshot',  # ğŸ”¥ V5.5: Kuyumcu fiyat snapshot'Ä± (disk backup!)
    'kurabak:backup:all'
]

# ======================================
# PUBLIC API
# ======================================

def get_cache(key: str) -> Optional[Any]:
    """
    ğŸ”¥ V4.8 FIX: Global client kullanÄ±mÄ±
    
    TRIPLE FALLBACK: Redis â†’ RAM â†’ Disk
    """
    client = redis_wrapper.get_client()
    
    # 1. Redis Denemesi
    if client:
        try:
            data = client.get(key)
            if data:
                return json.loads(data)
        except Exception as e:
            logger.warning(f"âš ï¸ Redis Okuma HatasÄ±: {e}")
    
    # 2. RAM Denemesi
    ram_data = ram_cache.get(key)
    if ram_data:
        return ram_data
    
    # 3. Disk Denemesi
    if key in CRITICAL_KEYS:
        logger.warning(f"ğŸ”¥ [{key}] Redis ve RAM'de yok, DISK'ten yÃ¼kleniyor...")
        disk_data = disk_backup.load(key)
        if disk_data:
            logger.info(f"âœ… [{key}] Disk'ten baÅŸarÄ±yla kurtarÄ±ldÄ±!")
            ram_cache.set(key, disk_data, ttl=0)
            return disk_data
    
    return None


def set_cache(key: str, data: Any, ttl: int = 300, force_disk_backup: bool = False) -> bool:
    """
    ğŸ”¥ V4.8 FIX: Disk backup optimize edildi
    
    Cache'e veri yazar + SADECE force_disk_backup=True ise disk'e yazar
    
    Args:
        key: Cache key
        data: Veri
        ttl: TTL (saniye, 0=sÃ¼resiz)
        force_disk_backup: True ise kritik key'leri disk'e de yaz
    """
    success = False
    
    try:
        json_data = json.dumps(data, default=str)
    except Exception as e:
        logger.error(f"âŒ JSON Serialization HatasÄ±: {e}")
        return False

    client = redis_wrapper.get_client()

    # 1. Redis Yazma
    if client:
        try:
            if ttl and ttl > 0:
                client.setex(key, ttl, json_data)
            else:
                client.set(key, json_data)
            success = True
        except Exception as e:
            logger.error(f"âŒ Redis Yazma HatasÄ±: {e}")

    # 2. RAM Yazma
    ram_cache.set(key, data, ttl)
    
    # 3. ğŸ”¥ V4.8: DISK BACKUP - Sadece force_disk_backup=True ise!
    if force_disk_backup and key in CRITICAL_KEYS:
        disk_backup.save(key, data)
        logger.debug(f"ğŸ’¾ [{key}] Disk'e yedeklendi")
    
    return success or True


def incr_cache(key: str, ttl: int = 0) -> int:
    """
    Atomik increment (Redis INCR veya RAM thread-safe)
    """
    client = redis_wrapper.get_client()
    
    if client:
        try:
            new_value = client.incr(key)
            
            if ttl > 0 and new_value == 1:
                client.expire(key, ttl)
            
            return new_value
            
        except Exception as e:
            logger.warning(f"âš ï¸ Redis INCR hatasÄ±: {e}")
    
    return ram_cache.incr(key, ttl)


def cache_exists(key: str) -> bool:
    """Key var mÄ± kontrol et (Redis â†’ RAM â†’ Disk)"""
    client = redis_wrapper.get_client()
    
    if client:
        try:
            return bool(client.exists(key))
        except Exception as e:
            logger.warning(f"âš ï¸ Redis EXISTS hatasÄ±: {e}")
    
    if ram_cache.exists(key):
        return True
    
    if key in CRITICAL_KEYS:
        return disk_backup.load(key) is not None
    
    return False


def delete_cache(key: str) -> bool:
    """Key'i sil (Redis + RAM + Disk)"""
    success = False
    client = redis_wrapper.get_client()
    
    if client:
        try:
            client.delete(key)
            success = True
        except Exception as e:
            logger.warning(f"âš ï¸ Redis DELETE hatasÄ±: {e}")
    
    ram_cache.delete(key)
    
    if key in CRITICAL_KEYS:
        disk_backup.delete(key)
    
    return success or True


def get_cache_keys(pattern: str = "*"):
    """Pattern'e uyan tÃ¼m key'leri dÃ¶ndÃ¼r"""
    client = redis_wrapper.get_client()
    
    if client:
        try:
            return [k.decode() if isinstance(k, bytes) else k 
                    for k in client.keys(pattern)]
        except Exception as e:
            logger.warning(f"âš ï¸ Redis KEYS hatasÄ±: {e}")
    
    ram_keys = ram_cache.keys(pattern)
    disk_keys = disk_backup.list_keys()
    
    all_keys = set(ram_keys + disk_keys)
    
    if pattern != "*":
        import fnmatch
        all_keys = {k for k in all_keys if fnmatch.fnmatch(k, pattern)}
    
    return list(all_keys)


def flush_all_cache() -> bool:
    """TÃœM cache'i temizle (Redis + RAM + Disk)"""
    success = False
    client = redis_wrapper.get_client()
    
    if client:
        try:
            client.flushall()
            logger.warning("ğŸ§¹ Redis tamamen temizlendi!")
            success = True
        except Exception as e:
            logger.error(f"âŒ Redis FLUSHALL hatasÄ±: {e}")
    
    ram_cache._cache.clear()
    logger.warning("ğŸ§¹ RAM Cache temizlendi!")
    
    for key in CRITICAL_KEYS:
        disk_backup.delete(key)
    logger.warning("ğŸ§¹ Disk Backup temizlendi!")
    
    return success or True


def cleanup_old_disk_backups(max_age_days: int = 7) -> dict:
    """ğŸ§¹ Eski disk backup'larÄ±nÄ± temizle"""
    before_stats = disk_backup.get_backup_stats()
    deleted_count = disk_backup.cleanup_old_backups(max_age_days)
    after_stats = disk_backup.get_backup_stats()
    
    return {
        'deleted_count': deleted_count,
        'before_stats': before_stats,
        'after_stats': after_stats
    }


def get_disk_backup_stats() -> dict:
    """ğŸ“Š Disk backup istatistiklerini getir"""
    return disk_backup.get_backup_stats()


def get_redis_client():
    """
    Redis client'Ä± dÃ¶ndÃ¼r (FCM notification iÃ§in)
    """
    return redis_wrapper.get_client()


def recover_from_disk():
    """Uygulama baÅŸlatÄ±lÄ±rken disk'ten kritik verileri yÃ¼kle"""
    logger.info("ğŸ”„ Disk'ten veri kurtarma kontrolÃ¼ baÅŸlatÄ±lÄ±yor...")
    
    recovered_count = 0
    
    for key in CRITICAL_KEYS:
        if not get_cache(key):
            disk_data = disk_backup.load(key)
            if disk_data:
                logger.info(f"ğŸ’¾ [{key}] Disk'ten kurtarÄ±ldÄ± ve RAM'e yÃ¼klendi")
                ram_cache.set(key, disk_data, ttl=0)
                recovered_count += 1
    
    if recovered_count > 0:
        logger.info(f"âœ… {recovered_count} adet veri disk'ten baÅŸarÄ±yla kurtarÄ±ldÄ±!")
    else:
        logger.info("â„¹ï¸ KurtarÄ±lacak veri bulunamadÄ± (Normal durum)")

recover_from_disk()


# ======================================
# ğŸ”¥ V5.6: SCHEDULER LOCK (circular import fix)
# Buraya taÅŸÄ±ndÄ±: app.py â†’ maintenance_service.py â†’ app.py dÃ¶ngÃ¼sÃ¼ kÄ±rÄ±ldÄ±.
# app.py ve maintenance_service.py her ikisi de buradan import eder.
# ======================================

SCHEDULER_LOCK_KEY = "kurabak:scheduler:lock"
SCHEDULER_LOCK_TTL = 120  # 2 dakika â€” worker her 60s'de yeniler, Ã§Ã¶kerse 120s'de kalkar


def renew_scheduler_lock():
    """
    Scheduler'Ä±n hÃ¢lÃ¢ yaÅŸadÄ±ÄŸÄ±nÄ± Redis'e bildirir.
    maintenance_service.py iÃ§indeki worker_job her Ã§alÄ±ÅŸmasÄ±nda (60s) bunu Ã§aÄŸÄ±rÄ±r.
    Sunucu Ã§Ã¶kerse SCHEDULER_LOCK_TTL sonunda lock otomatik kalkar,
    yeni Render worker'Ä± devralÄ±r.
    """
    try:
        client = get_redis_client()
        if client:
            client.set(SCHEDULER_LOCK_KEY, os.getpid(), ex=SCHEDULER_LOCK_TTL)
    except Exception:
        pass  # Lock yenileme kritik deÄŸil, sessizce geÃ§
